{
  "hash": "9cd0e47305e4e1c401a5d795e0092ab1",
  "result": {
    "markdown": "---\ntitle: \"Bootstrap again\"\noutput: html_notebook\n---\n\n\n\n## packages\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(bootstrap)\nlibrary(rsample)\nlibrary(conflicted)\nconflict_prefer(\"filter\", \"dplyr\") \n```\n:::\n\n\n\n## Is my sampling distribution normal enough?\n\n- Recall the IRS data that we used as a motivation for the sign test:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(irs, aes(x=Time))+geom_histogram(bins=10)\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-3-1.pdf)\n:::\n:::\n\n\n\n- We said that a $t$ procedure for the mean would not be a good idea because the distribution is skewed.\n\n## What *actually* matters\n\n- It's not the distribution of the *data* that has to be approx normal (for a $t$ procedure).\n- What matters is the *sampling distribution of the sample mean*.\n- If the sample size is large enough, the sampling distribution will be normal enough even if the data distribution is not.\n  - This is why we had to consider the sample size as well as the shape.\n- But how do we know whether this is the case or not? We only have *one* sample.\n\n## The (nonparametric) bootstrap\n\n- Typically, our sample will be reasonably representative of the population.\n- Idea: pretend the sample *is* the population, and sample from it *with replacement*.\n- Calculate test statistic, and repeat many times.\n- This gives an idea of how our statistic might vary in repeated samples: that is, its sampling distribution.\n- Called the **bootstrap distribution** of the test statistic.\n- If the bootstrap distribution is approx normal, infer that the true sampling distribution also approx normal, therefore inference about the mean such as $t$ is good enough.\n- If not, we should be more careful.\n\n## Why it works\n\n- We typically estimate population parameters by using the corresponding sample thing: eg. estimate population mean using sample mean.\n- This called **plug-in principle**.\n- The fraction of sample values less than a value $x$ called the **empirical distribution function** (as a function of $x$).\n- By plug-in principle, the empirical distribution function is an estimate of the population CDF.\n- In this sense, the sample *is* an estimate of the population, and so sampling from it is an estimate of sampling from the population.\n\n## Bootstrapping the IRS data\n\n- Sampling with replacement is done like this (the default sample size is as long as the original data):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nboot=sample(irs$Time, replace=T)\nmean(boot)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 182.5667\n```\n:::\n:::\n\n\n\n- That's one bootstrapped mean. We need a whole bunch.\n- Use the same idea as for simulating power:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell hash='bootstrap-again_cache/beamer/bootstrap-again-6_653cda2a4c66a4f8dd1607bb6dfb46ac'}\n\n```{.r .cell-code}\nrerun(1000, sample(irs$Time, replace=T)) %>% \n  map_dbl(~mean(.)) -> means\n```\n:::\n\n\n\n## Sampling distribution of sample mean\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tibble(means), aes(x=means))+geom_histogram(bins=20)\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-7-1.pdf)\n:::\n:::\n\n\n\n## Comments\n\nThis is not so bad: a long right tail, maybe:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tibble(means), aes(sample=means))+\n  stat_qq()+stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-8-1.pdf)\n:::\n:::\n\n\n\nor not so much.\n\n## Confidence interval from the bootstrap distribution\n\nThere are two ways (at least):\n\n- percentile bootstrap interval: take the 2.5 and 97.5 percentiles (to get the middle 95%). This is easy, but not always the best:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(b_p=quantile(means, c(0.025, 0.975)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%    97.5% \n159.5292 244.1075 \n```\n:::\n:::\n\n\n\n- bootstrap $t$: use the SD of the bootstrapped sampling distribution as the SE of the estimator of the mean and make a $t$ interval:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn=length(irs$Time)\nt_star=qt(0.975, n-1)\n(b_t=mean(means)+c(-1, 1)*t_star*sd(means))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 157.9020 245.4334\n```\n:::\n:::\n\n\n\n## Comparing\n\n- get ordinary $t$ interval:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_names=c(\"LCL\", \"UCL\")\no_t=t.test(irs$Time)$conf.int\n```\n:::\n\n\n\n\n- Compare the 2 bootstrap intervals with the ordinary $t$-interval: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(limit=my_names, o_t, b_t, b_p)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 4\n  limit   o_t   b_t   b_p\n  <chr> <dbl> <dbl> <dbl>\n1 LCL    155.  158.  160.\n2 UCL    247.  245.  244.\n```\n:::\n:::\n\n\n\n- The bootstrap $t$ and the ordinary $t$ are very close\n- The percentile bootstrap interval is noticeably shorter (common) and higher (skewness).\n  \n## Which to prefer?\n\n- If the intervals agree, then they are all good.\n- If they disagree, they are all bad! \n- In that case, use BCA interval (over).\n\n  \n## Bias correction and acceleration\n\n- this from \n\"An introduction to the bootstrap\", by\nBrad Efron and Robert J. Tibshirani.\n- there is way of correcting the CI for skewness in the bootstrap distribution, called the BCa method\n- complicated (see the Efron and Tibshirani book), but implemented in `bootstrap` package.\n\n## Run this on the IRS data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbca=bcanon(irs$Time, 1000, mean)\nbca$confpoints\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     alpha bca point\n[1,] 0.025  162.8000\n[2,] 0.050  166.8000\n[3,] 0.100  174.4667\n[4,] 0.160  179.5667\n[5,] 0.840  224.6000\n[6,] 0.900  234.3000\n[7,] 0.950  247.4333\n[8,] 0.975  256.3000\n```\n:::\n:::\n\n\n\n## use 2.5% and 97.5% points for CI\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbca$confpoints %>% as_tibble() %>% \n  filter(alpha %in% c(0.025, 0.975)) %>% \n  pull(`bca point`) -> b_bca\nb_bca\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 162.8 256.3\n```\n:::\n:::\n\n\n\n## Comparing\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(limit=my_names, o_t, b_t, b_p, b_bca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 5\n  limit   o_t   b_t   b_p b_bca\n  <chr> <dbl> <dbl> <dbl> <dbl>\n1 LCL    155.  158.  160.  163.\n2 UCL    247.  245.  244.  256.\n```\n:::\n:::\n\n\n\n- The BCA interval says that the mean should be estimated even higher than the bootstrap percentile interval does. \n- The BCA interval is the one to trust.\n\n\n## Bootstrapping the correlation\n\nRecall the soap data:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl=\"http://ritsokiguess.site/datafiles/soap.txt\"\nsoap=read_delim(url,\" \")\n```\n:::\n\n\n\n## The data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(soap, aes(x=speed, y=scrap, colour=line))+\n  geom_point()+geom_smooth(method=\"lm\", se=F)\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-17-1.pdf)\n:::\n:::\n\n\n\n## Comments\n\n- Line B produces less scrap for any given speed.\n- For line B, estimate the correlation between speed and scrap (with a confidence interval.)\n\n## Extract the line B data; standard correlation test \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsoap %>% filter(line==\"b\") -> line_b\nwith(line_b, cor.test(speed, scrap))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tPearson's product-moment correlation\n\ndata:  speed and scrap\nt = 15.829, df = 10, p-value = 2.083e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.9302445 0.9947166\nsample estimates:\n      cor \n0.9806224 \n```\n:::\n:::\n\n\n\n\n\n\n## Bootstrapping a correlation\n\n- Sample from data with replacement, but have to keep the `speed`-`scrap` *pairs* together\n- Sample *rows* at random, then take the variable values that belong to those rows:\n\n*** REDO THIS ***\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrerun(1000, sample(1:nrow(line_b), replace=T)) %>%\n  map(~slice(line_b, .)) %>% \n  map_dbl(~with(.,cor(speed, scrap))) -> cors\n```\n:::\n\n\n\n## A picture of this \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tibble(cors), aes(x=cors))+geom_histogram(bins=15)\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-21-1.pdf)\n:::\n:::\n\n\n\n## Comments and next steps\n\n- This is very left-skewed.\n- Bootstrap percentile interval is:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(b_p=quantile(cors, c(0.025, 0.975)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     2.5%     97.5% \n0.9443861 0.9960207 \n```\n:::\n:::\n\n\n\n- We probably need the BCA interval instead.\n\n## Getting the BCA interval 1/2\n\n- To use `bcanon`, write a function that takes a vector of row numbers and returns the correlation between `speed` and `scrap` for those rows:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta=function(rows, d) {\n  d %>% slice(rows) %>% with(., cor(speed, scrap))\n}\ntheta(1:3, line_b)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9928971\n```\n:::\n\n```{.r .cell-code}\nline_b %>% slice(1:3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 4\n   case scrap speed line \n  <dbl> <dbl> <dbl> <chr>\n1    16   140   105 b    \n2    17   277   215 b    \n3    18   384   270 b    \n```\n:::\n:::\n\n\n\n- That looks about right.\n\n## Getting the BCA interval 2/2\n\n- Inputs to `bcanon` are now:\n  - row numbers (1 through 12 in our case: 12 rows in `line_b`)\n  - number of bootstrap samples\n  - the function we just wrote\n  - the data frame:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoints=bcanon(1:12, 1000, theta, line_b)$confpoints\npoints %>% as_tibble() %>% \n  filter(alpha %in% c(0.025, 0.975)) %>% \n  pull(`bca point`) -> b_bca\nb_bca\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9281762 0.9945016\n```\n:::\n:::\n\n\n\n## Comparing the results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(limit=my_names, o_c, b_p, b_bca)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 4\n  limit   o_c   b_p b_bca\n  <chr> <dbl> <dbl> <dbl>\n1 LCL   0.930 0.944 0.928\n2 UCL   0.995 0.996 0.995\n```\n:::\n:::\n\n\n\n- The bootstrap percentile interval doesn't go down far enough. \n- The BCA interval seems to do a better job than the ordinary `cor.test` interval in capturing the skewness of the distribution.\n\n## A problem\n\nConsider this example: samples of UK and Ontario (Canada) children, and their journey times to school, in minutes:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url=\"http://ritsokiguess.site/datafiles/to-school.csv\"\nto_school=read_csv(my_url)\nto_school\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 80 x 2\n   traveltime location\n        <dbl> <chr>   \n 1         30 Ontario \n 2         10 Ontario \n 3          8 Ontario \n 4         30 Ontario \n 5          5 Ontario \n 6          8 Ontario \n 7          7 Ontario \n 8         15 Ontario \n 9         10 Ontario \n10         35 Ontario \n# i 70 more rows\n```\n:::\n\n```{.r .cell-code}\nto_school %>% count(location)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n  location     n\n  <chr>    <int>\n1 Ontario     40\n2 UK          40\n```\n:::\n:::\n\n\n\nWe want to compare the mean journey times in the two different places. This is a two-sample sitation, and if we are not careful with the bootstrap, things will go wrong:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample(1:nrow(to_school), replace=T) %>% slice(to_school, .) %>% count(location)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n  location     n\n  <chr>    <int>\n1 Ontario     46\n2 UK          34\n```\n:::\n:::\n\n\n\nOur original samples were 40 from each location, but by randomly resampling rows, we probably *don't* get 40 from each. We need to draw \"stratified resamples\" to ensure that we get 40 from each place. This is hard to organize with the build-it-yourself bootstrap. To make things easier, we use the `rsample` package, but then we have to worry about handling the results.\n\n## Automating the bootstrap\n\nLet's go back to our IRS data for a moment:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nirs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30 x 1\n    Time\n   <dbl>\n 1    91\n 2    64\n 3   243\n 4   167\n 5   123\n 6    65\n 7    71\n 8   204\n 9   110\n10   178\n# i 20 more rows\n```\n:::\n:::\n\n\n\nWhat happens if we use `rsample`  to resample from these? Let's just do a few to start. `rsample` has a function `bootstraps` that does this:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd <- bootstraps(irs, times=1000) \nd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bootstrap sampling \n# A tibble: 1,000 x 2\n   splits          id           \n   <list>          <chr>        \n 1 <split [30/8]>  Bootstrap0001\n 2 <split [30/10]> Bootstrap0002\n 3 <split [30/15]> Bootstrap0003\n 4 <split [30/8]>  Bootstrap0004\n 5 <split [30/11]> Bootstrap0005\n 6 <split [30/9]>  Bootstrap0006\n 7 <split [30/13]> Bootstrap0007\n 8 <split [30/11]> Bootstrap0008\n 9 <split [30/12]> Bootstrap0009\n10 <split [30/11]> Bootstrap0010\n# i 990 more rows\n```\n:::\n:::\n\n\n\nEach of those things in `splits` is one bootstrap resample. To get at the things in them, we use `analysis`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% mutate(sample=map(splits, ~analysis(.))) -> dd\ndd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bootstrap sampling \n# A tibble: 1,000 x 3\n   splits          id            sample             \n   <list>          <chr>         <list>             \n 1 <split [30/8]>  Bootstrap0001 <spc_tbl_ [30 x 1]>\n 2 <split [30/10]> Bootstrap0002 <spc_tbl_ [30 x 1]>\n 3 <split [30/15]> Bootstrap0003 <spc_tbl_ [30 x 1]>\n 4 <split [30/8]>  Bootstrap0004 <spc_tbl_ [30 x 1]>\n 5 <split [30/11]> Bootstrap0005 <spc_tbl_ [30 x 1]>\n 6 <split [30/9]>  Bootstrap0006 <spc_tbl_ [30 x 1]>\n 7 <split [30/13]> Bootstrap0007 <spc_tbl_ [30 x 1]>\n 8 <split [30/11]> Bootstrap0008 <spc_tbl_ [30 x 1]>\n 9 <split [30/12]> Bootstrap0009 <spc_tbl_ [30 x 1]>\n10 <split [30/11]> Bootstrap0010 <spc_tbl_ [30 x 1]>\n# i 990 more rows\n```\n:::\n:::\n\n\n\nand then `unnest` the actual samples to see them:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndd %>% unnest(sample)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30,000 x 3\n   splits         id             Time\n   <list>         <chr>         <dbl>\n 1 <split [30/8]> Bootstrap0001    84\n 2 <split [30/8]> Bootstrap0001   264\n 3 <split [30/8]> Bootstrap0001   302\n 4 <split [30/8]> Bootstrap0001    64\n 5 <split [30/8]> Bootstrap0001   142\n 6 <split [30/8]> Bootstrap0001   527\n 7 <split [30/8]> Bootstrap0001   264\n 8 <split [30/8]> Bootstrap0001   209\n 9 <split [30/8]> Bootstrap0001   243\n10 <split [30/8]> Bootstrap0001   527\n# i 29,990 more rows\n```\n:::\n:::\n\n\n\nThe values in `Time` are the resampled-with-replacement times to fill in the form.\n\nWhat we cared about here was the bootstrap distribution of the sample mean, so that for each of the samples in `dd` we need to find the mean `Time` in it:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndd %>% mutate(the_mean=map_dbl(sample, ~mean(.$Time))) -> ddd\nddd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# Bootstrap sampling \n# A tibble: 1,000 x 4\n   splits          id            sample              the_mean\n   <list>          <chr>         <list>                 <dbl>\n 1 <split [30/8]>  Bootstrap0001 <spc_tbl_ [30 x 1]>     234.\n 2 <split [30/10]> Bootstrap0002 <spc_tbl_ [30 x 1]>     200.\n 3 <split [30/15]> Bootstrap0003 <spc_tbl_ [30 x 1]>     175.\n 4 <split [30/8]>  Bootstrap0004 <spc_tbl_ [30 x 1]>     212.\n 5 <split [30/11]> Bootstrap0005 <spc_tbl_ [30 x 1]>     233.\n 6 <split [30/9]>  Bootstrap0006 <spc_tbl_ [30 x 1]>     211.\n 7 <split [30/13]> Bootstrap0007 <spc_tbl_ [30 x 1]>     230.\n 8 <split [30/11]> Bootstrap0008 <spc_tbl_ [30 x 1]>     175.\n 9 <split [30/12]> Bootstrap0009 <spc_tbl_ [30 x 1]>     216.\n10 <split [30/11]> Bootstrap0010 <spc_tbl_ [30 x 1]>     181.\n# i 990 more rows\n```\n:::\n:::\n\n\n\nand for example make a histogram of them, to see how normal this is:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ddd, aes(x=the_mean))+geom_histogram(bins=10)\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-33-1.pdf)\n:::\n:::\n\n\n\nThis actually looks pretty normal:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(ddd, aes(sample=the_mean))+stat_qq()+stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-34-1.pdf)\n:::\n:::\n\n\n\nall of which suggests that the $t$-interval for the mean:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwith(irs, t.test(Time))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  Time\nt = 8.9035, df = 29, p-value = 8.589e-10\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 155.0081 247.4585\nsample estimates:\nmean of x \n 201.2333 \n```\n:::\n:::\n\n\n\nand some kind of bootstrap interval for the mean, say the percentile-based one:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(ddd$the_mean, c(0.025, 0.975))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n    2.5%    97.5% \n159.0658 246.3008 \n```\n:::\n:::\n\n\n\nwon't be all that far apart.\n\nHow do I get the BCa interval from this output? First write a function that gets the mean Time from given rows of a data frame:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntheta <- function(rows, d) {\n  d %>% slice(rows) %>% \n    with(., mean(Time))\n}\nirs %>% slice(1:3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 x 1\n   Time\n  <dbl>\n1    91\n2    64\n3   243\n```\n:::\n\n```{.r .cell-code}\ntheta(1:3, irs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 132.6667\n```\n:::\n:::\n\n\n\ncheck. And then feed into `bcanon` these things:\n\n- row numbers of data frame that we want to use (all of them)\n- number of ... oh, but this uses the original data, or rows thereof\n\ncan I do \"stratified resampling\"? [Yes](https://gist.github.com/ramhiser/8b5ffd0ffbfbf1f49e71bbbd330bf72d)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nto_school %>% \n  group_by(location) %>% \n  sample_frac(replace=T) %>% \n  count(location)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 2\n# Groups:   location [2]\n  location     n\n  <chr>    <int>\n1 Ontario     40\n2 UK          40\n```\n:::\n:::\n\n\n\nthat seems to work, but I want to try it on groups of different sizes\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngroups <- tribble(\n  ~group, ~y,\n  \"A\", 1,\n  \"A\", 2,\n  \"B\", 3,\n  \"B\", 4,\n  \"B\", 5\n)\ngroups\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 2\n  group     y\n  <chr> <dbl>\n1 A         1\n2 A         2\n3 B         3\n4 B         4\n5 B         5\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngroups %>% \n  group_by(group) %>% \n  sample_frac(replace=T)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 2\n# Groups:   group [2]\n  group     y\n  <chr> <dbl>\n1 A         1\n2 A         2\n3 B         4\n4 B         3\n5 B         4\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nresample_by_group <- function(d, var, gp) {\n    d %>% group_by({{ gp }}) %>% \n    sample_frac(replace=T)\n}\ngroups %>% resample_by_group(var=y, gp=group)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 5 x 2\n# Groups:   group [2]\n  group     y\n  <chr> <dbl>\n1 A         1\n2 A         1\n3 B         5\n4 B         5\n5 B         4\n```\n:::\n:::\n\n\n\nsecond step: difference in means between (evidently two) grooups\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean_diff=function(d, var, gp) {\n  d %>% group_by({{ gp }}) %>% \n    summarize(m=mean({{ var }})) %>% pull(m) -> v\n  v[1]-v[2]\n}\nmean_diff(groups, y, group)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2.5\n```\n:::\n:::\n\n\n\nbootstrap it\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrerun(1000, resample_by_group(groups, y, group)) %>% \n  map_dbl(~mean_diff(., y, group)) -> means\n```\n:::\n\n\n\nvis\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tibble(means), aes(x=means))+geom_histogram()\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-44-1.pdf)\n:::\n:::\n\n\n\nor even\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntibble(means) %>% count(means)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 19 x 2\n   means     n\n   <dbl> <int>\n 1 -4       14\n 2 -3.67    25\n 3 -3.5     17\n 4 -3.33    51\n 5 -3.17    57\n 6 -3       68\n 7 -2.83   107\n 8 -2.67    30\n 9 -2.67    64\n10 -2.5    128\n11 -2.33    27\n12 -2.33    56\n13 -2.17   115\n14 -2       76\n15 -1.83    57\n16 -1.67    47\n17 -1.5     23\n18 -1.33    25\n19 -1       13\n```\n:::\n:::\n\n\n\n\nthis distribution is discrete but more or less normal:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tibble(means), aes(sample=means))+stat_qq()+stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-46-1.pdf)\n:::\n:::\n\n\n\n\n\nnow do it on travel times: this is wrong, because I have to sample the rows properly\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrerun(1000, resample_by_group(to_school, traveltime, location)) %>% \n  map_dbl(~mean_diff(., traveltime, location)) -> means\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tibble(means), aes(x=means))+geom_histogram(bins=20)\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-48-1.pdf)\n:::\n:::\n\n\n\nnormal quantile plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(tibble(means), aes(sample=means))+stat_qq()+stat_qq_line()\n```\n\n::: {.cell-output-display}\n![](bootstrap-again_files/figure-beamer/bootstrap-again-49-1.pdf)\n:::\n:::\n\n\n\nok, but that still doesn't solve bcanon. (I think, don't solve that.)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}
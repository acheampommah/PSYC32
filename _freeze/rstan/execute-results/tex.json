{
  "hash": "07c1f01260297318b6fc86453a78e542",
  "result": {
    "markdown": "---\ntitle: \"Bayesian Statistics with Stan\"\n---\n\n\n\n\n## Packages for this section\n\nInstallation instructions for the last three of these are below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(cmdstanr)\nlibrary(posterior)\nlibrary(bayesplot)\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n## Installation 1/2\n\n- `cmdstanr`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"cmdstanr\", \n                 repos = c(\"https://mc-stan.org/r-packages/\", \n                           getOption(\"repos\")))\n```\n:::\n\n\n\n- `posterior` and `bayesplot`, from the same place:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"posterior\", \n                 repos = c(\"https://mc-stan.org/r-packages/\", \n                            getOption(\"repos\")))\ninstall.packages(\"bayesplot\", \n                 repos = c(\"https://mc-stan.org/r-packages/\", \n                            getOption(\"repos\")))\n```\n:::\n\n\n\n## Installation 2/2\n\nThen, to check that you have the C++ stuff needed to compile Stan code:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncheck_cmdstan_toolchain()\n```\n:::\n\n\n\nand then:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall_cmdstan(cores = 4)\n```\n:::\n\n\n\nIf you happen to know how many cores (processors) your computer has, insert the appropriate number. (My laptop has 4 and my desktop 6.)\n\nAll of this is done once. If you have problems, go [here (link)](https://mc-stan.org/cmdstanr/articles/cmdstanr.html).\n\n## Bayesian and frequentist inference 1/2\n\n- The inference philosophy that we have learned so far says that:\n  - parameters to be estimated are *fixed* but *unknown*\n  - Data random; if we took another sample we'd get different data.\n- This is called \"frequentist\" or \"repeated-sampling\" inference.\n\n\n## Bayesian and frequentist inference 2/2\n\n- Bayesian inference says:\n  - *parameters* are random, *data* is *given*\n- Ingredients:\n  - **prior distribution**: distribution of parameters before seeing data.\n  - **likelihood**: model for data if the parameters are known \n  - **posterior distribution**: distribution of parameters *after* seeing data.\n  \n## Distribution of parameters\n\n- Instead of having a point or interval estimate of a parameter, we have an entire distribution\n- so in Bayesian statistics we can talk about eg.\n  - probability that a parameter is bigger than some value\n  - probability that a parameter is close to some value\n  - probability that one parameter is bigger than another\n  \n- Name comes from Bayes' Theorem, which here says\n\n> posterior is proportional to likelihood times prior\n\n- more discussion about this is in \n[**a blog post**](http://ritsokiguess.site/docs/2018/02/28/working-my-way-back-to-you-a-re-investigation-of-rstan/). \n\n## An example\n\n- Suppose we have these (integer) observations:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n(x <- c(0, 4, 3, 6, 3, 3, 2, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 4 3 6 3 3 2 4\n```\n:::\n:::\n\n\n\n- Suppose we believe that these come from a Poisson distribution with a mean $\\lambda$ that we want to estimate.\n- We need a prior distribution for $\\lambda$. I will (for some reason) take a $Weibull$ distribution with parameters 1.1 and 6, that has quartiles 2 and 6. Normally this would come from your knowledge of the data-generating *process*.\n- The Poisson likelihood can be written down (see over).\n\n## Some algebra\n\n- We have $n=8$ observations $x_i$, so the Poisson likelihood is proportional to\n\n$$ \\prod_{i=1}^n e^{-\\lambda} \\lambda^{x_i} = e^{-n\\lambda} \\lambda^S, $$\nwhere $S=\\sum_{i=1}^n x_i$. \n\n- then you write the Weibull prior density (as a function of $\\lambda$):\n\n$$ C (\\lambda/6)^{0.1} e^{-(\\lambda/6)^{1.1}}  $$\nwhere $C$ is a constant.\n\n- and then you multiply these together and try to recognize the distributional form. Only, here you can't. The powers 0.1 and 1.1 get in the way.\n\n## Sampling from the posterior distribution\n\n- Wouldn't it be nice if we could just *sample* from the posterior distribution? Then we would be able to compute it as accurately as we want.\n\n- Metropolis and Hastings: devise a Markov chain (C62) whose limiting distribution is the posterior you want, and then sample from that Markov chain (easy), allowing enough time to get close enough to the limiting distribution.\n\n- Stan: uses a modern variant that is more efficient (called Hamiltonian Monte Carlo), implemented in R packages `cmdstanr`.\n\n- Write Stan code in a file, compile it and sample from it.\n\n## Components of Stan code: the model\n\n```\nmodel {\n  // likelihood\n  x ~ poisson(lambda);\n}\n```\n\nThis is how you say \"$X$ has a Poisson distribution with mean $\\lambda$\". **Note that lines of Stan code have semicolons on the end.**\n\n## Components of Stan code: the prior distribution\n\n```\nmodel {\n  // prior\n  lambda ~ weibull(1.1, 6);\n  // likelihood\n  x ~ poisson(lambda);\n}\n```\n\n## Components of Stan code: data and parameters \n\n- first in the Stan code:\n\n```\ndata {\n  int x[8];\n}\n\nparameters {\n  real<lower=0> lambda;\n}\n```\n\n## Compile and sample from the model 1/2\n\n- compile\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1 <- cmdstan_model(\"poisson1.stan\")\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n// Estimating Poisson mean\n\ndata {\n  int x[8];\n}\n\nparameters {\n  real<lower=0> lambda;\n}\n\nmodel {\n  // prior\n  lambda ~ weibull(1.1, 6);\n  // likelihood\n  x ~ poisson(lambda);\n}\n```\n:::\n:::\n\n\n\n## Compile and sample from the model 2/2\n\n- set up data\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1_data <- list(x = x)\npoisson1_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$x\n[1] 0 4 3 6 3 3 2 4\n```\n:::\n:::\n\n\n\n- sample\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1_fit <- poisson1$sample(data = poisson1_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.0 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.0 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.0 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.0 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.0 seconds.\nTotal execution time: 0.7 seconds.\n```\n:::\n:::\n\n\n\n\n\n\n## The output\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson1_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n variable mean median   sd  mad   q5  q95 rhat ess_bulk ess_tail\n   lp__   3.77   4.02 0.65 0.32 2.46 4.26 1.00     2053     2619\n   lambda 3.18   3.14 0.61 0.62 2.24 4.23 1.00     1332     1922\n```\n:::\n:::\n\n\n\n\n## Comments\n\n- This summarizes the posterior distribution of $\\lambda$\n- the posterior mean is 3.19\n- with a 90% posterior interval of 2.25 to 4.33.\n- The probability that $\\lambda$ is between these two values really is 90%.\n\n## Making the code more general\n\n- The coder in you is probably offended by hard-coding the sample size and the parameters of the prior distribution. More generally:\n\n```\ndata {\n  int<lower=1> n;\n  real<lower=0> a;\n  real<lower=0> b;\n  int x[n];\n}\n...\nmodel {\n// prior\nlambda ~ weibull(a, b);\n// likelihood\nx ~ poisson(lambda);\n}\n```\n\n## Set up again and sample:\n\n- Compile again:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2 <- cmdstan_model(\"poisson2.stan\")\n```\n:::\n\n\n\n\n- set up the data again including the new things we need:\n\n\\footnotesize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2_data <- list(x = x, n = length(x), a = 1.1, b = 6)\npoisson2_data\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n$x\n[1] 0 4 3 6 3 3 2 4\n\n$n\n[1] 8\n\n$a\n[1] 1.1\n\n$b\n[1] 6\n```\n:::\n:::\n\n\n\n\\normalsize\n\n\n## Sample again\n\nOutput should be the same (to within randomness):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2_fit <- poisson2$sample(data = poisson2_data)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n variable mean median   sd  mad   q5  q95 rhat ess_bulk ess_tail\n   lp__   3.75   4.03 0.68 0.32 2.33 4.26 1.00     1662     2462\n   lambda 3.19   3.13 0.63 0.62 2.23 4.30 1.00     1516     1720\n```\n:::\n:::\n\n\n\n## Picture of posterior\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_hist(poisson2_fit$draws(\"lambda\"), binwidth = 0.25)\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-16-1.pdf)\n:::\n:::\n\n\n\n\n## Extracting actual sampled values\n\nA little awkward at first:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2_fit$draws()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A draws_array: 1000 iterations, 4 chains, and 2 variables\n, , variable = lp__\n\n         chain\niteration   1   2   3   4\n        1 4.2 4.3 4.2 3.0\n        2 4.2 4.2 4.2 4.3\n        3 4.2 4.3 4.2 4.2\n        4 4.2 4.1 3.9 4.2\n        5 4.2 4.1 3.3 3.6\n\n, , variable = lambda\n\n         chain\niteration   1   2   3   4\n        1 3.4 3.2 3.4 4.3\n        2 3.0 3.4 3.5 3.2\n        3 3.4 3.2 3.3 3.1\n        4 3.4 3.6 2.7 3.1\n        5 3.0 3.6 2.4 2.5\n\n# ... with 995 more iterations\n```\n:::\n:::\n\n\n\nA 3-dimensional array. A dataframe would be much better.\n\n## Sampled values as dataframe\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(poisson2_fit$draws()) %>% \n  as_tibble() -> poisson2_draws\npoisson2_draws\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4,000 x 5\n    lp__ lambda .chain .iteration .draw\n   <dbl>  <dbl>  <int>      <int> <int>\n 1  4.22   3.38      1          1     1\n 2  4.18   2.96      1          2     2\n 3  4.20   3.41      1          3     3\n 4  4.21   3.38      1          4     4\n 5  4.23   3.04      1          5     5\n 6  4.07   2.83      1          6     6\n 7  4.15   2.91      1          7     7\n 8  2.95   4.31      1          8     8\n 9  2.85   4.36      1          9     9\n10  4.08   3.58      1         10    10\n# i 3,990 more rows\n```\n:::\n:::\n\n\n\n\n\n## Posterior predictive distribution\n\n- Another use for the actual sampled values is to see what kind of *response* values we might get in the future. This should look something like our data. For a Poisson distribution, the response values are integers: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npoisson2_draws %>% \n  rowwise() %>% \n  mutate(xsim = rpois(1, lambda)) -> d\n```\n:::\n\n\n\n## The simulated posterior distribution (in `xsim`)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd %>% select(lambda, xsim)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4,000 x 2\n# Rowwise: \n   lambda  xsim\n    <dbl> <int>\n 1   3.38     4\n 2   2.96     1\n 3   3.41     3\n 4   3.38     2\n 5   3.04     3\n 6   2.83     2\n 7   2.91     5\n 8   4.31     7\n 9   4.36     5\n10   3.58     3\n# i 3,990 more rows\n```\n:::\n:::\n\n\n\n\n\n## Comparison\n\nOur actual data values were these:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0 4 3 6 3 3 2 4\n```\n:::\n:::\n\n\n\n- None of these are very unlikely according to our posterior predictive distribution, so our model is believable. \n- Or make a plot: a bar chart with the data on it as well (over):\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(d, aes(x = xsim)) + geom_bar() +\n  geom_dotplot(data = tibble(x), aes(x = x), binwidth = 1) +\n  scale_y_continuous(NULL, breaks = NULL) -> g\n```\n:::\n\n\n\n- This also shows that the distribution of the data conforms well enough to the posterior predictive distribution (over).\n\n## The plot \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-22-1.pdf)\n:::\n:::\n\n\n\n## Do they have the same distribution? \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nqqplot(d$xsim, x, plot.it = FALSE) %>% as_tibble() -> dd\ndd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 8 x 2\n      x     y\n  <dbl> <dbl>\n1     0     0\n2     1     2\n3     2     3\n4     3     3\n5     3     3\n6     4     4\n7     5     4\n8    14     6\n```\n:::\n:::\n\n\n\n## The plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(dd, aes(x=x, y=y)) + geom_point()\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/unnamed-chunk-4-1.pdf)\n:::\n:::\n\n\n\nthe observed zero is a bit too small compared to expected (from the posterior), but the other points seem pretty well on a line.\n\n## Analysis of variance, the Bayesian way\n\nRecall the jumping rats data: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_url <- \n  \"http://ritsokiguess.site/datafiles/jumping.txt\"\nrats0 <- read_delim(my_url, \" \")\nrats0\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30 x 2\n   group   density\n   <chr>     <dbl>\n 1 Control     611\n 2 Control     621\n 3 Control     614\n 4 Control     593\n 5 Control     593\n 6 Control     653\n 7 Control     600\n 8 Control     554\n 9 Control     603\n10 Control     569\n# i 20 more rows\n```\n:::\n:::\n\n\n\n## Our aims here\n\n- Estimate the mean bone density of all rats under each of the experimental conditions\n- Model: given the group means, each observation normally distributed with common variance $\\sigma^2$\n- Three parameters to estimate, plus the common variance.\n- Obtain posterior distributions for the group means.\n- Ask whether the posterior distributions of these means are sufficiently different.\n\n## Numbering the groups\n\n- Stan doesn't handle categorical variables (everything is `real` or `int`).\n- Turn the groups into group *numbers* first.\n- Take opportunity to put groups in logical order:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrats0 %>% mutate(\n  group_fct = fct_inorder(group),\n  group_no = as.integer(group_fct)\n) -> rats\nrats\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 30 x 4\n   group   density group_fct group_no\n   <chr>     <dbl> <fct>        <int>\n 1 Control     611 Control          1\n 2 Control     621 Control          1\n 3 Control     614 Control          1\n 4 Control     593 Control          1\n 5 Control     593 Control          1\n 6 Control     653 Control          1\n 7 Control     600 Control          1\n 8 Control     554 Control          1\n 9 Control     603 Control          1\n10 Control     569 Control          1\n# i 20 more rows\n```\n:::\n:::\n\n\n\n## Plotting the data 1/2\n\nMost obviously, boxplots: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rats, aes(x = group_fct, y = density)) + \n  geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-26-1.pdf)\n:::\n:::\n\n\n\n## Plotting the data 2/2\n\nAnother way: density plot (smoothed out histogram); can distinguish groups by colours: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(rats, aes(x = density, fill = group_fct)) +\n  geom_density(alpha = 0.6)\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/density_plot-1.pdf)\n:::\n:::\n\n\n\n\n## The procedure\n\n- For each observation, find out which (numeric) group it belongs to, \n- then model it as having a normal distribution with that group's mean and the common variance.\n- Stan does `for` loops.\n\n## The model part\n\nSuppose we have `n_obs` observations:\n\n```\nmodel {\n  // likelihood\n  for (i in 1:n_obs) {\n    g = group_no[i];\n    density[i] ~ normal(mu[g], sigma);\n  }\n}\n```\n\n## The variables here {.scrollable}\n\n- `n_obs` is data.\n- `g` is a temporary integer variable only used here\n- `i` is only used in the loop (integer) and does not need to be declared\n- `density` is data, a real vector of length `n_obs`\n- `mu` is a parameter, a real vector of length 3 (3 groups)\n- `sigma` is a real parameter\n\n`mu` and `sigma` need prior distributions: \n\n  - for `mu`, each component independently normal with mean 600 and SD 50 (my guess at how big and variable they will be)\n  - for `sigma`, chi-squared with 50 df (my guess at typical amount of variability from obs to obs)\n\n## Complete the `model` section:\n\n```\nmodel {\n  int g;\n  // priors\n  mu ~ normal(600, 50);\n  sigma ~ chi_square(50);\n  // likelihood\n  for (i in 1:n_obs) {\n    g = group_no[i];\n    density[i] ~ normal(mu[g], sigma);\n  }\n}\n```\n\n## Parameters\n\nThe elements of `mu`, one per group, and also `sigma`, scalar, lower limit zero:\n\n```\nparameters {\n  real mu[n_group];\n  real<lower=0> sigma;\n}\n```\n\n- Declare `sigma` to have lower limit zero here, so that the sampling runs smoothly. \n- declare `n_group` in data section \n\n## Data\n\nEverything else:\n\n```\ndata {\n  int n_obs;\n  int n_group;\n  real density[n_obs];\n  int<lower=1, upper=n_group> group_no[n_obs];\n}\n```\n\n## Compile\n\nArrange these in order data, parameters, model in file `anova.stan`, then:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova <- cmdstan_model(\"anova.stan\")\n```\n:::\n\n\n\n\n## Set up data and sample\n\nSupply values for *everything* declared in `data`: \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_data <- list(\n  n_obs = 30,\n  n_group = 3,\n  density = rats$density,\n  group_no = rats$group_no\n)\nanova_fit <- anova$sample(data = anova_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRunning MCMC with 4 sequential chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 0.1 seconds.\nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 0.1 seconds.\nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 0.1 seconds.\nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 0.1 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 0.1 seconds.\nTotal execution time: 0.5 seconds.\n```\n:::\n:::\n\n\n\n## Check that the sampling worked properly \n\n\\scriptsize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_fit$cmdstan_diagnose()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nProcessing csv files: /tmp/RtmpXFqlRF/anova-202308101148-1-359d71.csv, /tmp/RtmpXFqlRF/anova-202308101148-2-359d71.csv, /tmp/RtmpXFqlRF/anova-202308101148-3-359d71.csv, /tmp/RtmpXFqlRF/anova-202308101148-4-359d71.csv\n\nChecking sampler transitions treedepth.\nTreedepth satisfactory for all transitions.\n\nChecking sampler transitions for divergences.\nNo divergent transitions found.\n\nChecking E-BFMI - sampler transitions HMC potential energy.\nE-BFMI satisfactory.\n\nEffective sample size satisfactory.\n\nSplit R-hat values satisfactory all parameters.\n\nProcessing complete, no problems detected.\n```\n:::\n:::\n\n\n\\normalsize\n\n## Look at the results\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n    lp__  -41.00 -40.69 1.45 1.27 -43.75 -39.31 1.00     1905     2457\n    mu[1] 601.04 600.90 8.96 8.71 586.20 615.63 1.00     4176     2739\n    mu[2] 612.05 612.14 8.96 8.85 597.18 626.99 1.00     3775     2687\n    mu[3] 637.58 637.54 8.85 8.63 622.98 652.06 1.00     4500     3112\n    sigma  28.45  28.09 4.16 4.10  22.19  35.87 1.00     3256     2874\n```\n:::\n:::\n\n\n\n## Comments\n\n- The posterior 95% intervals for control (group 1) and highjump (group 3) do not quite overlap, suggesting that these exercise groups really are different.\n- Bayesian approach does not normally do tests: look at posterior distributions and decide whether they are different enough to be worth treating as different.\n\n## Plotting the posterior distributions for the `mu`\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmcmc_hist(anova_fit$draws(\"mu\"), binwidth = 5)\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-29-1.pdf)\n:::\n:::\n\n\n\n\n## Extract the sampled values \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas_draws_df(anova_fit$draws()) %>% as_tibble() -> anova_draws\nanova_draws\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 4,000 x 8\n    lp__ `mu[1]` `mu[2]` `mu[3]` sigma .chain .iteration .draw\n   <dbl>   <dbl>   <dbl>   <dbl> <dbl>  <int>      <int> <int>\n 1 -39.8    600.    612.    638.  32.0      1          1     1\n 2 -42.2    607.    630.    629.  25.6      1          2     2\n 3 -41.0    602.    620.    624.  32.2      1          3     3\n 4 -42.9    612.    591.    633.  29.9      1          4     4\n 5 -43.7    609.    585.    637.  31.7      1          5     5\n 6 -42.5    616.    616.    653.  33.1      1          6     6\n 7 -41.7    598.    617.    654.  33.3      1          7     7\n 8 -40.2    608.    615.    649.  27.3      1          8     8\n 9 -42.1    582.    612.    627.  30.6      1          9     9\n10 -40.7    593.    616.    651.  29.8      1         10    10\n# i 3,990 more rows\n```\n:::\n:::\n\n\n\n## estimated probability that $\\mu_3 > \\mu_1$ \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_draws %>% \n  count(`mu[3]`>`mu[1]`) %>% \n  mutate(prob = n/sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 x 3\n  `\\`mu[3]\\` > \\`mu[1]\\``     n    prob\n  <lgl>                   <int>   <dbl>\n1 FALSE                      11 0.00275\n2 TRUE                     3989 0.997  \n```\n:::\n:::\n\n\nHigh jumping group almost certainly has larger mean than control group.\n\n## More organizing\n\n- for another plot\n  - make longer\n  - give `group` values their proper names back \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanova_draws %>% \n  pivot_longer(starts_with(\"mu\"), \n               names_to = \"group\", \n               values_to = \"bone_density\") %>% \n  mutate(group = fct_recode(group,\n    Control = \"mu[1]\",\n    Lowjump = \"mu[2]\",\n    Highjump = \"mu[3]\"\n  )) -> sims\n```\n:::\n\n\n\n## What we have now:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsims \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12,000 x 7\n    lp__ sigma .chain .iteration .draw group    bone_density\n   <dbl> <dbl>  <int>      <int> <int> <fct>           <dbl>\n 1 -39.8  32.0      1          1     1 Control          600.\n 2 -39.8  32.0      1          1     1 Lowjump          612.\n 3 -39.8  32.0      1          1     1 Highjump         638.\n 4 -42.2  25.6      1          2     2 Control          607.\n 5 -42.2  25.6      1          2     2 Lowjump          630.\n 6 -42.2  25.6      1          2     2 Highjump         629.\n 7 -41.0  32.2      1          3     3 Control          602.\n 8 -41.0  32.2      1          3     3 Lowjump          620.\n 9 -41.0  32.2      1          3     3 Highjump         624.\n10 -42.9  29.9      1          4     4 Control          612.\n# i 11,990 more rows\n```\n:::\n:::\n\n\n\n\n## Density plots of posterior mean distributions\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sims, aes(x = bone_density, fill = group)) + \n  geom_density(alpha = 0.6)\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-33-1.pdf)\n:::\n:::\n\n\n\n## Posterior predictive distributions\n\nRandomly sample from posterior means and SDs in `sims`. There are 12000 rows in `sims`:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsims %>% mutate(sim_data = rnorm(12000, bone_density, sigma)) -> ppd\nppd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 12,000 x 8\n    lp__ sigma .chain .iteration .draw group    bone_density sim_data\n   <dbl> <dbl>  <int>      <int> <int> <fct>           <dbl>    <dbl>\n 1 -39.8  32.0      1          1     1 Control          600.     611.\n 2 -39.8  32.0      1          1     1 Lowjump          612.     625.\n 3 -39.8  32.0      1          1     1 Highjump         638.     618.\n 4 -42.2  25.6      1          2     2 Control          607.     584.\n 5 -42.2  25.6      1          2     2 Lowjump          630.     633.\n 6 -42.2  25.6      1          2     2 Highjump         629.     627.\n 7 -41.0  32.2      1          3     3 Control          602.     657.\n 8 -41.0  32.2      1          3     3 Lowjump          620.     589.\n 9 -41.0  32.2      1          3     3 Highjump         624.     660.\n10 -42.9  29.9      1          4     4 Control          612.     617.\n# i 11,990 more rows\n```\n:::\n:::\n\n\n\n## Compare posterior predictive distribution with actual data\n\n- Check that the model works: distributions of data similar to what we'd predict\n- Idea: make plots of posterior predictive distribution, and plot actual data as points on them\n- Use facets, one for each treatment group:\n\n\\scriptsize\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_binwidth <- 15\nggplot(ppd, aes(x = sim_data)) +\n  geom_histogram(binwidth = my_binwidth) +\n  geom_dotplot(\n    data = rats, aes(x = density),\n    binwidth = my_binwidth\n  ) +\n  facet_wrap(~group) +\n  scale_y_continuous(NULL, breaks = NULL) -> g\n```\n:::\n\n\n\n\\normalsize\n\n- See (over) that the data values are mainly in the middle of the predictive distributions.\n- Even for the control group that had outliers. \n\n## The plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ng\n```\n\n::: {.cell-output-display}\n![](rstan_files/figure-beamer/rstan-35-1.pdf)\n:::\n:::\n\n\n\n## Extensions \n\n- if you want a different model other than normal, change distribution in `model` section\n- if you want to allow unequal spreads, create `sigma[n_group]` and in model `density[i] ~ normal(mu[g], sigma[g]);`\n- Stan will work just fine after you recompile\n- very flexible.\n- Typical modelling strategy: start simple, add complexity as warranted by data.\n\n\n\n",
    "supporting": [
      "rstan_files/figure-beamer"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}